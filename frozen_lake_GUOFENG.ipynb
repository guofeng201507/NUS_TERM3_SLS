{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from dp import policy_iteration, value_iteration\n",
    "\n",
    "# Action mappings\n",
    "action_mapping = {\n",
    "    0: '\\u2191',  # UP\n",
    "    1: '\\u2192',  # RIGHT\n",
    "    2: '\\u2193',  # DOWN\n",
    "    3: '\\u2190',  # LEFT\n",
    "}\n",
    "\n",
    "\n",
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "\n",
    "        while not terminated:\n",
    "\n",
    "            # Select best action to perform in a current state\n",
    "            action = np.argmax(policy[state])\n",
    "\n",
    "            # Perform an action an observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "\n",
    "    average_reward = total_reward / n_episodes\n",
    "\n",
    "    return wins, total_reward, average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Discrete(4)\n",
      "Discrete(64)\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake8x8-v0')\n",
    "\n",
    "environment.render()\n",
    "print(environment.action_space)  # Discrete()\n",
    "print(environment.observation_space)  # Discrete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 203 iterations.\n",
      "Evaluated 2 policies.\n",
      "\n",
      " Final policy derived using Policy Iteration:\n",
      "← ← ← ↓ ↓ ↓ ↓ ↓ ← ← ← ← ↓ ↓ ↓ ↓ ← ← ↑ ↑ ↓ ← ↓ ↓ ← ← ← → ↑ ↑ ↓ ↓ ← ← ↑ ↑ ↓ → ← ↓ ↑ ↑ ↑ → ← ↑ ↑ ↓ ↑ ↑ → ↑ ↑ ↑ ↑ ↓ ↑ → ↑ ↑ → → → ↑\n"
     ]
    }
   ],
   "source": [
    "iteration_name = 'Policy Iteration'\n",
    "iteration_func = policy_iteration\n",
    "\n",
    "    # Search for an optimal policy using policy iteration\n",
    "policy, V = iteration_func(environment.env)\n",
    "\n",
    "print(f'\\n Final policy derived using {iteration_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy, axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 18 iterations.\n",
      "Evaluated 2 policies.\n",
      "\n",
      " Final policy derived using Policy Iteration:\n",
      "→ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → ↓ ↓ ← ↓ ↓ → → → ↓ ↑ ↑ ↓ ← ↓ → ← ↓ ← → ↑ ↑ ↓ → ← ← ↑ ↑ ↓ → ← ↓ ↑ ↑ ↑ → ← ↑ ↑ ↓ ↑ ↑ → ↑ ↑ ↑ ↑ ↓ → → ↑ ↑ → → → ↑\n"
     ]
    }
   ],
   "source": [
    "policy2, V = iteration_func(environment.env, discount_factor = 0.5)\n",
    "\n",
    "print(f'\\n Final policy derived using {iteration_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy2, axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_iter(policy, n_episodes = 1000):\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration :: number of wins over 5000 episodes = 4129\n",
      "Policy Iteration :: average reward over 5000 episodes = 0.8258 \n",
      "\n",
      "\n",
      "Policy Iteration :: number of wins over 10000 episodes = 8323\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.8323 \n",
      "\n",
      "\n",
      "Policy Iteration :: number of wins over 5000 episodes = 2803\n",
      "Policy Iteration :: average reward over 5000 episodes = 0.5606 \n",
      "\n",
      "\n",
      "Policy Iteration :: number of wins over 10000 episodes = 5555\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.5555 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_policy_iter(policy, 5000)\n",
    "test_policy_iter(policy, 10000)\n",
    "\n",
    "test_policy_iter(policy2, 5000)\n",
    "test_policy_iter(policy2, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
